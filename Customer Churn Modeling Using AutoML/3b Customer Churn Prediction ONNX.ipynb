{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Predict Customer Churn\n",
        "The notebook ingests, prepares and then trains a model based on dataset that tracks customer activity in an ecommerce store. The goal is to for a given customer, predict whether there will be a churn or not. The model then will be converted to ONNX format and tracked by MLFlow.\n",
        "We will later use the ONNX model for inferencing in Azure Synapse SQL Pool using the new model scoring wizard.\n",
        "## Note:\n",
        "**Please note that this notebook was tested with Scikit-learn version 1.0.2 and Scikit-learn version 0.22.2.post1.**  \n",
        "If you need to downgrade your scikit-learn, uncomment the line in the below cell and run it. Once it completes running, restart the kernel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Importing libraries and setting up workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1658534225011
        }
      },
      "outputs": [],
      "source": [
        "import azureml.core\n",
        "from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.data.dataset_factory import TabularDatasetFactory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1658534225807
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from azure.storage.blob import ContainerClient, BlobClient\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "from copy import deepcopy\n",
        "import GlobalVariables as gv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1658534226751
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import LinkedService\n",
        "import datetime  \n",
        "from azureml.core import Workspace, LinkedService, SynapseWorkspaceLinkedServiceConfiguration\n",
        "\n",
        "# Azure Machine Learning workspace\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "linked_service = LinkedService.get(ws, 'synapselink1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Load data from datastore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1658534251992
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.get_by_name(workspace=ws, name='customer_churn_dataset', version='latest')\n",
        "\n",
        "df = dataset.to_pandas_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1658534262550
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df[\"Price\"] = pd.to_numeric(df[\"Price\"])\n",
        "df[\"Quantity\"] = pd.to_numeric(df[\"Quantity\"])\n",
        "df[\"Segment\"] = pd.to_numeric(df[\"Segment\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1658534262736
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 354345 entries, 0 to 354344\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count   Dtype  \n",
            "---  ------     --------------   -----  \n",
            " 0   Segment    354345 non-null  int64  \n",
            " 1   Quantity   354345 non-null  int64  \n",
            " 2   Price      354345 non-null  float64\n",
            " 3   StockCode  354345 non-null  object \n",
            "dtypes: float64(1), int64(2), object(1)\n",
            "memory usage: 10.8+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1658534263689
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.3, random_state=123)\n",
        "\n",
        "x_train = pd.DataFrame(train_df.drop(['Segment'], axis = 1))\n",
        "y_train = pd.DataFrame(train_df.iloc[:,train_df.columns.tolist().index('Segment')])\n",
        "\n",
        "x_test = pd.DataFrame(test_df.drop(['Segment'], axis = 1))\n",
        "y_test = pd.DataFrame(test_df.iloc[:,test_df.columns.tolist().index('Segment')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Save test dataset to be used in synapse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1658534265020
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "test_df.to_csv('customer_churn_test_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Train Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1658534286581
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('preprocessor',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('float',\n",
              "                                                  Pipeline(memory=None,\n",
              "                                                           steps=[('imputer',\n",
              "                                                                   SimpleImputer(add_indicator=False,\n",
              "                                                                                 copy=True,\n",
              "                                                                                 fill_value=None,\n",
              "                                                                                 missing_values=nan,\n",
              "                                                                                 strategy='median',\n",
              "                                                                                 verbose=0)),\n",
              "                                                                  ('scaler',\n",
              "                                                                   StandardScaler(copy=True,\n",
              "                                                                                  with_me...\n",
              "                                                                                 handle_unknown='ignore',\n",
              "                                                                                 sparse=True))],\n",
              "                                                           verbose=False),\n",
              "                                                  ['StockCode'])],\n",
              "                                   verbose=False)),\n",
              "                ('classifier',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "float_features = ['Price']\n",
        "float_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "integer_features = ['Quantity']\n",
        "integer_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "categorical_features = ['StockCode']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('float', float_transformer, float_features),\n",
        "        ('integer', integer_transformer, integer_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('classifier', LogisticRegression(solver='lbfgs'))])\n",
        "\n",
        "# Train the model\n",
        "clf.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1658534286752
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5542971101745936\n"
          ]
        }
      ],
      "source": [
        "# Evalute the model\n",
        "score = clf.score(x_test, y_test)\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Convert the model to ONNX format\n",
        "Currently, T-SQL scoring only supports ONNX model format (https://onnx.ai/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1658534286837
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The maximum opset needed by this model is only 11.\n",
            "The maximum opset needed by this model is only 1.\n"
          ]
        }
      ],
      "source": [
        "from skl2onnx import convert_sklearn\n",
        "from skl2onnx.common.data_types import FloatTensorType, Int64TensorType, DoubleTensorType, StringTensorType\n",
        "\n",
        "def convert_dataframe_schema(df, drop=None):\n",
        "    inputs = []\n",
        "    for k, v in zip(df.columns, df.dtypes):\n",
        "        if drop is not None and k in drop:\n",
        "            continue\n",
        "        if v == 'int64':\n",
        "            t = Int64TensorType([1, 1])\n",
        "        elif v == 'float32':\n",
        "            t = FloatTensorType([1, 1])\n",
        "        elif v == 'float64':\n",
        "            t = FloatTensorType([1, 1])\n",
        "        else:\n",
        "            t = StringTensorType([1, 1])\n",
        "        inputs.append((k, t))\n",
        "    return inputs\n",
        "\n",
        "model_inputs = convert_dataframe_schema(x_train)\n",
        "onnx_model = convert_sklearn(clf, \"customer_churn_predict\", model_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1658534287070
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Register the model with MLFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1658534295733
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.onnx\n",
        "\n",
        "from mlflow.models.signature import infer_signature\n",
        "\n",
        "experiment_name = 'customer_churn_predict_exp'\n",
        "artifact_path = 'ncustomer_churn_predict_artifact'\n",
        "\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "with mlflow.start_run() as run:\n",
        "    # Infer signature\n",
        "    input_sample = x_train.head(1)\n",
        "    output_sample = pd.DataFrame(columns=['output_label'], data=[1])\n",
        "    signature = infer_signature(input_sample, output_sample)\n",
        "\n",
        "    # Save the model to the outputs directory for capture\n",
        "    mlflow.onnx.log_model(onnx_model, artifact_path, signature=signature, input_example=input_sample)\n",
        "\n",
        "    # Register the model to AML model registry\n",
        "    mlflow.register_model('runs:/' + run.info.run_id + '/' + artifact_path, 'customer_churn_predict')\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
