{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Azure Form Recognizer\n",
        "\n",
        "Azure Form Recognizer is a cognitive service that uses machine learning technology to identify and extract key-value pairs and table data from form documents. It then outputs structured data that includes the relationships in the original file.\n",
        "\n",
        "    \n",
        "\n",
        "### Overview\n",
        "*Safety Incident Reports Dataset*: Raw unstructured data is fed into the pipeline in the form of electronically generated PDFs. These reports contain information about injuries that occurred at several locations belonging to a company. This data provides information on injury reports, including the nature, description, date, source and the name of the establishment where it happened. \n",
        "\n",
        "\n",
        "### Notebook Organization \n",
        "+ Fetch the injury report PDF files from a container under an azure storage account.\n",
        "\n",
        "+ Convert the PDF files to JSON by querying the azure trained form recognizer model using the REST API.\n",
        "\n",
        "+ Preprocess the JSON files to extract only relevant information.\n",
        "\n",
        "+ Push the JSON files to a container under an azure storage account."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Relevant Libraries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Please install this specific version of azure storage blob compatible with this notebook.\n",
        "!pip install azure-storage-blob==2.1.0\n",
        "\n",
        "# Import the required libraries\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "from azure.storage.blob import BlockBlobService\n",
        "import pprint\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import shutil\n",
        "import pickle\n",
        "import GlobalVariables as gv"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: azure-storage-blob==2.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (2.1.0)\nRequirement already satisfied: azure-common>=1.1.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-storage-blob==2.1.0) (1.1.25)\nRequirement already satisfied: azure-storage-common~=2.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-storage-blob==2.1.0) (2.1.0)\nRequirement already satisfied: python-dateutil in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-storage-common~=2.1->azure-storage-blob==2.1.0) (2.8.0)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-storage-common~=2.1->azure-storage-blob==2.1.0) (2.23.0)\nRequirement already satisfied: cryptography in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-storage-common~=2.1->azure-storage-blob==2.1.0) (2.7)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from python-dateutil->azure-storage-common~=2.1->azure-storage-blob==2.1.0) (1.14.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->azure-storage-common~=2.1->azure-storage-blob==2.1.0) (1.24.2)\nRequirement already satisfied: idna<3,>=2.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->azure-storage-common~=2.1->azure-storage-blob==2.1.0) (2.8)\nRequirement already satisfied: chardet<4,>=3.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->azure-storage-common~=2.1->azure-storage-blob==2.1.0) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->azure-storage-common~=2.1->azure-storage-blob==2.1.0) (2019.11.28)\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cryptography->azure-storage-common~=2.1->azure-storage-blob==2.1.0) (1.12.3)\nRequirement already satisfied: asn1crypto>=0.21.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cryptography->azure-storage-common~=2.1->azure-storage-blob==2.1.0) (1.0.1)\nRequirement already satisfied: pycparser in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=2.1->azure-storage-blob==2.1.0) (2.19)\n"
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Local Folders"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create local directories if they don't exist\n",
        "# *input_forms* contains all the pdf files to be converted to json\n",
        "if (not os.path.isdir(os.getcwd()+\"/input_forms\")):\n",
        "    os.makedirs(os.getcwd()+\"/input_forms\")\n",
        "# *output_json* will contain all the converted json files\n",
        "if (not os.path.isdir(os.getcwd()+\"/output_json\")):\n",
        "    os.makedirs(os.getcwd()+\"/output_json\")"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the PDF forms from a container in azure storage"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Downloads all PDF forms from a container named *incidentreport* to a local folder *input_forms*"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Downloading pdf files from a container named *incidentreport* to a local folder *input_forms*\n",
        "# Set up configs for blob storage\n",
        "STORAGE_ACCOUNT_NAME = gv.SAFFETY_INCIDENT_STORAGE_ACCOUNT_NAME\n",
        "STORAGE_ACCOUNT_ACCESS_KEY = gv.SAFFETY_INCIDENT_STORAGE_ACCOUNT_ACCESS_KEY\n",
        "STORAGE_CONTAINER_NAME = gv.SAFFETY_INCIDENT_STORAGE_CONTAINER_NAME\n",
        "\n",
        "# Instantiating a blob service object\n",
        "blob_service = BlockBlobService(STORAGE_ACCOUNT_NAME, STORAGE_ACCOUNT_ACCESS_KEY) \n",
        "\n",
        "blobs = blob_service.list_blobs(STORAGE_CONTAINER_NAME)\n",
        "# Downloading pdf files from the container *incidentreport* and storing them locally to *input_forms* folder\n",
        "for blob in blobs:\n",
        "    # Check if the blob.name is already present in the folder input_forms. If yes then continue\n",
        "    try:\n",
        "        with open('merged_log','rb') as f:\n",
        "            merged_files = pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "        merged_files = set()\n",
        "    # If file is already processed then continue to next file\n",
        "    if (blob.name in merged_files): \n",
        "        continue\n",
        "    download_file_path = os.path.join(os.getcwd(), \"input_forms\", blob.name)\n",
        "    blob_service.get_blob_to_path(STORAGE_CONTAINER_NAME, blob.name ,download_file_path)\n",
        "    merged_files.add(blob.name)\n",
        "    # Keep trace of all the processed files at the end of your script (to keep track later)\n",
        "    with open('merged_log', 'wb') as f:\n",
        "        pickle.dump(merged_files, f)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of forms to be converted to JSON\n",
        "files = [f for f in listdir(os.getcwd()+\"/input_forms\") if isfile(join(os.getcwd()+\"/input_forms\", f))]"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying the custom trained form recognizer model (PDF -> JSON)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Converts PDF -> JSON by querying the trained custom model.\n",
        "- Preprocess the JSON file and extract only the relevant information."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Endpoint parameters for querying the custom trained form-recognizer model to return the processed JSON\n",
        "# Processes PDF files one by one and return CLEAN JSON files\n",
        "endpoint = gv.SAFFETY_INCIDENT_ENDPOINT\n",
        "# Change if api key is expired\n",
        "apim_key = gv.SAFFETY_INCIDENT_APIM_KEY\n",
        "# This model is the one trained on 5 forms\n",
        "model_id = gv.SAFFETY_INCIDENT_MODELID\n",
        "post_url = endpoint + \"/formrecognizer/v2.0-preview/custom/models/%s/analyze\" % model_id\n",
        "files = [f for f in listdir(os.getcwd()+\"/input_forms\") if isfile(join(os.getcwd()+\"/input_forms\", f))]\n",
        "params = {\"includeTextDetails\": True}\n",
        "headers = {'Content-Type': 'application/pdf', 'Ocp-Apim-Subscription-Key': apim_key}\n",
        "\n",
        "local_path = os.path.join(os.getcwd(), \"input_forms//\")\n",
        "output_path = os.path.join(os.getcwd(), \"output_json//\")\n",
        "\n",
        "for file in files:\n",
        "    try:\n",
        "        with open('json_log','rb') as l:\n",
        "            json_files = pickle.load(l)\n",
        "    except FileNotFoundError:\n",
        "        json_files = set()\n",
        "    if (file in json_files): \n",
        "        continue\n",
        "    else:\n",
        "        with open(local_path+file, \"rb\") as f:\n",
        "            data_bytes = f.read()\n",
        "        \n",
        "    try:\n",
        "        resp = requests.post(url = post_url, data = data_bytes, headers = headers, params = params)\n",
        "        print('resp',resp)\n",
        "        if resp.status_code != 202:\n",
        "            print(\"POST analyze failed:\\n%s\" % json.dumps(resp.json()))\n",
        "            quit()\n",
        "        print(\"POST analyze succeeded:\\n%s\" % resp.headers)\n",
        "        get_url = resp.headers[\"operation-location\"]\n",
        "    except Exception as e:\n",
        "        print(\"POST analyze failed:\\n%s\" % str(e))\n",
        "        quit()\n",
        "     \n",
        "    n_tries = 15\n",
        "    n_try = 0\n",
        "    wait_sec = 5\n",
        "    max_wait_sec = 60\n",
        "    while n_try < n_tries:\n",
        "        try:\n",
        "            resp = requests.get(url = get_url, headers = {\"Ocp-Apim-Subscription-Key\": apim_key})\n",
        "            resp_json = resp.json()\n",
        "            if resp.status_code != 200:\n",
        "                print(\"GET analyze results failed:\\n%s\" % json.dumps(resp_json))\n",
        "                quit()\n",
        "            status = resp_json[\"status\"]\n",
        "            if status == \"succeeded\":\n",
        "                print(\"Analysis succeeded:\\n%s\" % file[:-4])\n",
        "                allkeys = resp_json['analyzeResult']['documentResults'][0]['fields'].keys()\n",
        "                new_dict = {}\n",
        "                for i in allkeys:\n",
        "                    if resp_json['analyzeResult']['documentResults'][0]['fields'][i] != None:\n",
        "                        key = i.replace(\" \", \"_\")\n",
        "                        new_dict[key] = resp_json['analyzeResult']['documentResults'][0]['fields'][i]['valueString']\n",
        "                    else:\n",
        "                        key = i.replace(\" \", \"_\")\n",
        "                        new_dict[key] = None\n",
        "                # Appending form url to json\n",
        "                new_dict['form_url'] = 'https://stcognitivesearch0001.blob.core.windows.net/formupload/' + file \n",
        "                with open(output_path+file[:-4]+\".json\", 'w') as outfile:\n",
        "                    json.dump(new_dict, outfile)\n",
        "                # Change the encoding of file in case of spanish forms. It will detected random characters.\n",
        "                with open(output_path+file[:-4]+\".json\", 'w', encoding='utf-8') as outfile:\n",
        "                    json.dump(new_dict, outfile, ensure_ascii=False)\n",
        "                # Once JSON is saved log it otherwise don't log it.\n",
        "                json_files.add(file)\n",
        "                with open('json_log', 'wb') as f:\n",
        "                    pickle.dump(json_files, f)\n",
        "\n",
        "                break\n",
        "            if status == \"failed\":\n",
        "                print(\"Analysis failed:\\n%s\" % json.dumps(resp_json))\n",
        "                quit()\n",
        "            # Analysis still running. Wait and retry.\n",
        "            time.sleep(wait_sec)\n",
        "            n_try += 1\n",
        "            wait_sec = min(2*wait_sec, max_wait_sec)     \n",
        "        except Exception as e:\n",
        "            msg = \"GET analyze results failed:\\n%s\" % str(e)\n",
        "            print(msg)\n",
        "            quit()"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload the JSON files to a cotainer\n",
        "\n",
        "- Upload JSON files from local folder *output_json* to the container *formrecogoutput*"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of converted JSON\n",
        "files = [f for f in listdir(os.getcwd()+\"/output_json\") if isfile(join(os.getcwd()+\"/output_json\", f))]"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Connect to the container for uploading the JSON files\n",
        "# Set up configs for blob storage\n",
        "STORAGE_ACCOUNT_NAME = gv.SAFFETY_INCIDENT_STORAGE_ACCOUNT_NAME\n",
        "STORAGE_ACCOUNT_ACCESS_KEY = gv.SAFFETY_INCIDENT_STORAGE_ACCOUNT_ACCESS_KEY\n",
        "# Upload the JSON files in this container\n",
        "STORAGE_CONTAINER_NAME = gv.SAFFETY_INCIDENT_STORAGE_FR\n",
        "# Instantiating a blob service object\n",
        "blob_service = BlockBlobService(STORAGE_ACCOUNT_NAME, STORAGE_ACCOUNT_ACCESS_KEY)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CPU times: user 384 µs, sys: 0 ns, total: 384 µs\nWall time: 390 µs\n"
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Upload JSON files from local folder *output_json* to the container *formrecogoutput*\n",
        "local_path = os.path.join(os.getcwd(), \"output_json\")\n",
        "# print(local_path)\n",
        "for files in os.listdir(local_path):\n",
        "#     print(os.path.join(local_path,files))\n",
        "    blob_service.create_blob_from_path(STORAGE_CONTAINER_NAME, files, os.path.join(local_path,files))"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}